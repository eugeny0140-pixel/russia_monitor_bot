import os
import time
import logging
import re
import feedparser
from datetime import datetime, timedelta
from urllib.parse import urlparse
from http.server import BaseHTTPRequestHandler, HTTPServer
import threading
import schedule
import requests
from deep_translator import GoogleTranslator, MyMemoryTranslator
from supabase import create_client

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ===
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("bot.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# === –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è ===
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_IDS = [cid.strip() for cid in os.getenv("CHANNEL_ID1", "").split(",") if cid.strip()]
if os.getenv("CHANNEL_ID2"):
    CHANNEL_IDS.append(os.getenv("CHANNEL_ID2").strip())

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

PORT = int(os.getenv("PORT", 10000))

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Supabase ===
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# === –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ ===
SOURCES = [
    {"name": "E3G", "rss": "https://www.e3g.org/feed/"},
    {"name": "Foreign Affairs", "rss": "https://www.foreignaffairs.com/rss.xml"},
    {"name": "Reuters Institute", "rss": "https://reutersinstitute.politics.ox.ac.uk/feed"},
    {"name": "Bruegel", "rss": "https://www.bruegel.org/rss"},
    {"name": "Chatham House", "rss": "https://www.chathamhouse.org/feed"},
    {"name": "CSIS", "rss": "https://www.csis.org/rss.xml"},
    {"name": "Atlantic Council", "rss": "https://www.atlanticcouncil.org/feed/"},
    {"name": "RAND Corporation", "rss": "https://www.rand.org/rss/recent.xml"},
    {"name": "CFR", "rss": "https://www.cfr.org/rss.xml"},
    {"name": "Carnegie Endowment", "rss": "https://carnegieendowment.org/rss"},
    {"name": "The Economist", "rss": "https://www.economist.com/rss/the_world_this_week_rss.xml"},
    {"name": "Bloomberg Politics", "rss": "https://www.bloomberg.com/politics/feeds/site.xml"},
]

# === –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ===
KEYWORDS = {
    'russia', 'ukraine', 'putin', 'kremlin', 'sanctions', 'gas', 'oil',
    'military', 'nato', 'eu', 'usa', 'europe', 'moscow', 'kiev', 'kyiv',
    'defense', 'war', 'geopolitic', 'energy', 'export', 'grain', 'black sea'
}

def translate_text(text: str, target="ru") -> str:
    if not text.strip():
        return ""
    try:
        return GoogleTranslator(source='auto', target=target).translate(text)
    except Exception as e:
        logger.warning(f"GoogleTranslate failed: {e}. Trying MyMemory.")
        try:
            return MyMemoryTranslator(source='auto', target=target).translate(text)
        except Exception as e2:
            logger.error(f"MyMemory failed too: {e2}. Returning original.")
            return text

def escape_markdown_v2(text: str) -> str:
    # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –¥–ª—è MarkdownV2
    escape_chars = r'_*[]()~`>#+-=|{}.!'
    for c in escape_chars:
        text = text.replace(c, '\\' + c)
    return text

def is_relevant(title: str, description: str) -> bool:
    text = (title + " " + description).lower()
    return any(kw in text for kw in KEYWORDS)

def is_generic_description(desc: str) -> bool:
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à–∞–±–ª–æ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
    generic_phrases = ["appeared first on", "read more", "click here", "¬©"]
    return any(phrase in desc for phrase in generic_phrases)

def send_to_telegram(prefix: str, title: str, lead: str, url: str):
    try:
        title_ru = translate_text(title)
        lead_ru = translate_text(lead)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
        message = f"{prefix}: {title_ru}\n\n{lead_ru}\n\n[–ò—Å—Ç–æ—á–Ω–∏–∫]({url})"
        message = escape_markdown_v2(message)

        for channel in CHANNEL_IDS:
            url_tg = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
            payload = {
                "chat_id": channel,
                "text": message,
                "parse_mode": "MarkdownV2",
                "disable_web_page_preview": False
            }
            resp = requests.post(url_tg, json=payload, timeout=10)
            if resp.status_code == 200:
                logger.info(f"‚úÖ Sent to {channel}: {title}")
            else:
                logger.error(f"‚ùå Telegram error: {resp.text}")
    except Exception as e:
        logger.exception(f"Failed sending message: {e}")

def article_already_sent(url: str) -> bool:
    try:
        response = supabase.table("published_articles").select("url").eq("url", url).execute()
        return len(response.data) > 0
    except Exception as e:
        logger.error(f"Supabase check error: {e}")
        return False  # –ù–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏ ‚Äî —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏

def mark_article_as_sent(url: str, title: str):
    try:
        supabase.table("published_articles").insert({"url": url, "title": title}).execute()
        logger.info(f"üìå Marked as sent: {url}")
    except Exception as e:
        logger.error(f"Supabase insert error: {e}")

def fetch_and_process():
    logger.info("üì° Starting feed check...")
    for source in SOURCES:
        name = source["name"]
        rss_url = source["rss"]
        prefix = name
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries:
                url = entry.get("link", "").strip()
                if not url:
                    continue

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏
                if article_already_sent(url):
                    continue

                title = entry.get("title", "").strip()
                desc = entry.get("summary", "").strip() or entry.get("description", "").strip()

                if not title or not desc:
                    continue

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à–∞–±–ª–æ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
                if is_generic_description(desc):
                    continue

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ
                if not is_relevant(title, desc):
                    continue

                # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü –∏–ª–∏ –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                lead = desc.split("\n")[0].split(". ")[0]
                if not lead.strip():
                    continue

                logger.info(f"üîç Found relevant: {title} ({url})")

                send_to_telegram(prefix, title, lead, url)
                mark_article_as_sent(url, title)

                time.sleep(1)  # –∏–∑–±–µ–≥–∞–µ–º —Å–ø–∞–º–∞

        except Exception as e:
            logger.error(f"Error processing {rss_url}: {e}")

    logger.info("‚úÖ Feed check complete.")

# === HTTP-—Å–µ—Ä–≤–µ—Ä –¥–ª—è Render ===
class HealthHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.send_header("Content-type", "text/plain")
        self.end_headers()
        self.wfile.write(b"OK")

def run_http_server():
    server = HTTPServer(("", PORT), HealthHandler)
    logger.info(f"üåê HTTP server running on port {PORT}")
    server.serve_forever()

# === –ó–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    logger.info("üöÄ Russia Monitor Bot starting...")

    # –ó–∞–ø—É—Å–∫–∞–µ–º HTTP-—Å–µ—Ä–≤–µ—Ä –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    http_thread = threading.Thread(target=run_http_server, daemon=True)
    http_thread.start()

    # –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ —Å—Ä–∞–∑—É
    fetch_and_process()

    # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫: –∫–∞–∂–¥—ã–µ 30 –º–∏–Ω—É—Ç
    schedule.every(30).minutes.do(fetch_and_process)

    # –ö–æ—Å—Ç—ã–ª—å –¥–ª—è cron-job: –∫–∞–∂–¥—ã–π —á–∞—Å –ø–∏–Ω–≥—É–µ–º "keep-alive"
    schedule.every().hour.do(lambda: logger.info("‚è∞ Cron heartbeat"))

    while True:
        schedule.run_pending()
        time.sleep(30)
